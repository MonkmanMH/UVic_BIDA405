---
title: "Documentation"
subtitle: ""
output:
  html_document:
    df_print: paged
---

<!-- 
This file by Martin Monkman is licensed under a Creative Commons Attribution 4.0 International License. 
-->

```{r setup}
library(tidyverse)
library(magrittr)

library(palmerpenguins)

```

> Document and comment your code for yourself as if you will need to understand it in 6 months. 
—Jenny Bryan


## Objectives

* Demonstrate understanding of the reasons for documentation and recordkeeping,

* Demonstrate understanding of the important elements to record, and

* Demonstrate the ability to create an artifact that captures the computing environment that a project was created in.


## Documentation and Recordkeeping

The reasons for documenting our workflow are similar to the reasons we would save and output our cleaned data file: 

* To make the project available to a collaborator. 

* To provide a record when we move to different jobs or projects (we would want others to do that for us!).

* To create a save point in our workflow, so that we can continue our work later but without having to go through all the steps to remember everything we've done.

![unfinished work](unfinished_work_400.png)

_Source: [monkeyuser.com](https://www.monkeyuser.com/2018/unfinished-work/), 2018-07-03_

As well, things change (the data, the sofware and hardware we use, the motivations for our work) and capturing the details about those things helps us make any necessary changes that might be required at a later date.



There doesn't seem to be a single-best how-to instruction on "documentation" in the context of data science. The term appears in the periphery of some of the textbooks and manuals on how to do data science, and there are some journal articles that address documentation in the context of reproducible research. It seems what advice there is tends to be prescriptive at a high level, and not overly descriptive of what makes good documentation.

In the journal article "Good enough practices in scientific computing", the authors write

> Good documentation is a key factor in software adoption, but in practice, people won't write comprehensive documentation until they have collaborators who will use it. They will, however, quickly see the point of a brief explanatory comment at the start of each script, so we have recommended that as a first step.
—Wilson, Bryan, et al.
^[Wilson, Bryan, et al., "Good enough practices in scientific computing", _PLOS Computational Biology_, 13:6, June 22, 2017](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510)

Similar sentiments are expressed in a recent blog post by Randy Au ^[["Let's Get Intentional About Documentation"](https://counting.substack.com/p/lets-get-intentional-about-documentation), 2020-09-29] expresses on key idea in the title, and builds on it: we need to be intentional in our actions around documentation (or as he argues, a better term might be "recordkeeping").

>Recordkeeping is keeping records, notes, and artifacts around to **help in creating documentation in the future.** It’s done by the people involved in doing the actual work saving things and making clarifications that only they really understand, with no expectation that someone else needs to understand everything. The material collected is primarily intended for use to write the actual documentation. Think of it as preserving raw data for future use.

I like the way this is framed. "Documentation" then becomes a fully-realized user manual, suitable for complex and recurring projects with many different contributors, and that might be considered mission critical. A lot of data science work doesn't meet those criteria, so what we need isn't "documentation" but the notes that someone could use to build the documentation.

If it's a one-off project, you can probably have a minimum (but not zero!) recordkeeping. But a project that repeats on a regular schedule (whether daily or annually) should have more robust recordkeeping, and potentially some documentation.

And it's important to think about this at the beginning—maybe even _before_ the beginning. Then it gets built into the process and your daily workflow, and becomes part of the work. For example, record the source of your data as you're downloading it, or make comments about why you chose a particular function as soon as you incorporate the function into your script. These habits will not only capture what you're thinking at the time, but will also encourage you to think about the "why" of your decisions—and perhaps lead you to a different choice.

### Principles for a research compendia

For those working in an academic setting, Marwick, Boettiger and Mullen have defined three generic principles for what should be included in a research compendia. These principles can also be adapted to other settings:

* Organize the files according to the prevailing standards. 

* Maintain a clear separation of data, method, and output.

* Specify the computational environment that was used.

^[Ben Marwick, Carl Boettiger & Lincoln Mullen, "Packaging Data Analytical Work Reproducibly Using R (and Friends)", _The American Statistician_, 72: Special Issue on Data Science (2018)], p.81(https://www.tandfonline.com/doi/abs/10.1080/00031305.2017.1375986?journalCode=utas20)

They also provide some concrete examples of that a file structure might look like; note that the structure explicitly follows that of an R package. Their figure showing a "medium compendia" is below:

![Medium research compendia](utas_a_1375986_f0003_b.gif)

In this example, there are elements that might not be relevant for every project (for example, a code and data license). But note that there's a clear separation of the data and the code that generates the analysis. As well, this structure is well-supported by the "project" approach, where what is shown here as the "COMPENDIUM" folder would be the root folder of your project. Everything else is self-contained within it.





### Elements of effective recordkeeping

There are three things to capture in your recordkeeping:

1. The why and how of the decisions made.

1. How things work together. 

1. How to make changes.



So what are the elements of effective recordkeeping? Some of the things to capture:

* A brief description of the project, analysis, and research objective.

* Diagrams that capture logic, flow, and inter-relationships.

* Detailed information about the data source:

  - Name of organization that collected the data
  
  - Name and contact information of individual at that organization
  
  - Date of download or extract
  
  - Web links
  
  - Original file names
  
  - Links to additional documentation about the data, such as the data dictionary

* A high-level statement about _how_ the analysis was done

  - "This analysis used the daily data from 2010 through 2019 to calculate average per month volumes, which were then used in an ARIMA model to develop a 6 month forecast".


* Details about the analysis and code:

  - The aim is to help people understand the code. "Document interfaces and reasons, not implementations." ^[Wilson et al., "Best Practices in Scientific Computing", _PLOS Biology_, 12:1, January 2014](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745)

  - Things that informed decisions ("I chose this statistical test because..." or "The variable date of birth was transformed into 'Age on July 1, 2020' because...")
  
  - The instruction list—descriptive text to describe what chunks of code are doing, but not how they are doing it

  - The data dictionary or code book

  - A map of the file folder structure


* Details about the software and packages used:

  - Versions and (if available) dates


## Some more details

### README files

Having a README file in the root folder of your project is a good place to capture a great deal of the important general information about the project.

But having another README in the data folder wouldn't hurt either.

The source ["Art of README"](https://github.com/noffle/art-of-readme) has some good tips on creating one. Although it's aimed at people writing computer programs and modules, the lessons apply to data science projects as well.

>Your job is to
>
>1. tell them what it is (with context)
>1. show them what it looks like in action
>1. show them how they use it
>1. tell them any other relevant details

Some other tips the authors provide:

* "The ideal README is as short as it can be without being any shorter."

* "Aggressively linkify!"


Another source that describes a README for data files is the ["Guide to writing "readme" style metadata"](https://data.research.cornell.edu/content/readme#:~:text=A%20readme%20file%20provides%20information,when%20sharing%20or%20publishing%20data.) written by the Research Data Management Service Group at Cornell University.

The SFBrigage, a San Francisco volunteer civic tech group, has created a data science specific README outline for their projects: https://github.com/sfbrigade/data-science-wg/blob/master/dswg_project_resources/Project-README-template.md

The basic minimum structure they have is:

* Project Intro/Objective

* Project Description

  - A missing component is "Data source" 

* Team (with contact information)

In an analytic project, you might also want to include the high-level findings. This might be something akin to an abstract in an academic journal article, or an executive summary in a business or government report.


Some other good tips can be found in [Make a README](https://www.makeareadme.com/) and [Readme Best Practices](https://github.com/jehna/readme-best-practices#readme)



### The instruction list

The instruction list is descriptive text that describes what chunks of code are doing.

In "How to Share Data for Collaboration", Ellis and Leek suggest something that is essentiall a high-level outline of what the code is doing. Here's a rephrase of their first two steps:  

* Step 1: Take the raw data file, run summarize code with parameters _a_ = 1, _b_ = 2

* Step 2: Run the code separately for each sample 

To then apply Knuth's "Literate Programming" ideas, each step would then be followed by the code to carry out that step. The R Markdown format is ideal for this sort of work; the text description would be followed by a chunk of R code, followed by another text description / R chunk pair, and so on.




### The data dictionary or code book

Here are the three minimum components of a data dictionary:

>1. information about the variables (including units!) in the dataset not contained in the tidy data,
>
>1. information about the summary choices made, and
>
>1. information about the experimental study design [or the data source]
>
>—Ellis & Leek ^[Ellis and Leek, ["How to Share Data for Collaboration"](https://peerj.com/preprints/3139/)]



## Tools to help with documentation


### The computational environment

#### The verion of R

The functions `R.Version()` or `version()` will both return a number of details about the version of R that you're running.

```{r}
R.Version()
#
rversion <- R.Version()
rversion$version.string
# or
version

```

#### The version of the packages

{annotater} - annotates package load calls

This RStudio add-in adds information about the packages in your script

```{r}

library(Lahman) 

```


#### File folder structure

The package {fs} has a function `dir_tree()` that will create a dendogram of the file folder structure and files in your project folder.

```{r}

library(fs)

fs::dir_tree(path = "E:/project_template", recurse = TRUE)

```

{fs} has a variety of other functions for programming queries and changes to your file and folder structures, including returning file size, and changes such as file copying, deleting.

The reference page for {fs}: https://fs.r-lib.org/


### Data dictionary

Web app:
https://rubenarslan.ocpu.io/codebook/www/


RStudio Addin:
```{r}
gapminder <- gapminder::gapminder
```

R code:





Reference material for {codebook}:

* https://www.rdocumentation.org/packages/codebook/versions/0.9.2

* https://journals.sagepub.com/doi/full/10.1177/2515245919838783

* https://rubenarslan.github.io/codebook/



Another R package that can be used to create a data dictionary is {dataMeta}:

https://bookdown.org/lyzhang10/lzhang_r_tips_book/how-to-make-a-simple-data-dictionary.html

https://cran.r-project.org/web/packages/dataMeta/vignettes/dataMeta_Vignette.html


Some other resources I found but didn't use:

https://research.tue.nl/files/145367570/Data_Dictionary_Group_Tutorial.pdf

http://optimumsportsperformance.com/blog/creating-a-data-dictionary-function-in-r/



#### {packrat} .

[{packrat}](https://rstudio.github.io/packrat/) is a "dependency management system" that not only documents which version of R and the packages your project uses, but stores them and takes a snapshot of them so that other users have a record of your software.





## REFERENCES

Randy Au, ["Let's Get Intentional About Documentation"](https://counting.substack.com/p/lets-get-intentional-about-documentation), 2020-09-29

Randy Au, ["Data Science Practice 101: Always Leave An Analysis Paper Trail"](https://towardsdatascience.com/data-science-practice-101-always-leave-an-analysis-paper-trail-cc17079fae5a), 2019-04-16

Buttery and Whitaker, chapter 7.6

Donald Knuth, _Literate Programming_

* [wikipedia article](https://en.wikipedia.org/wiki/Literate_programming)







