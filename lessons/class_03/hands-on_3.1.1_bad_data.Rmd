---
title: 'Validation and cleaning'
output:
  html_document:
    df_print: paged
---

<!-- 
This file by Martin Monkman is licensed under a Creative Commons Attribution 4.0 International License. 
-->


# Getting started

We've read in our raw data. The next step is to make sure that the data is what we expect, and what we need for our analysis. That is , what is the data's _quality_? 

A different way this is sometimes approached is whether or not the data is _dirty_ and needs _cleaning_.

What are common dirty data problems?

* Column headers are values instead of variable names, or there are duplicate variable names.

* Multiple data types in a column. For example, some records are numbers and some are character strings—and this could be entirely correct, but if some number entry includes commas as thousands separators, it might be stored as a character.

* Multiple variables are stored in one column (examples include first name & last name, city & province or state).

* Misspellings (typing errors) 

* Value in wrong field (e.g. country entered into `city` field).

* Irregularities in unit of measurement (in an international survey or form, `salary` completed by respondents in the values of their local currency).

* Default values in the place of missing values.

> Dr Davis Lawrence, director of safety-literature database the SafeLit Foundation...tells me that 'in most US states the quality of police crash reports is at best poor for use as a research tool. ... Data-quality checks were rare and when quality was evaluated it was found wanting. For example, in Louisiana for most crashes in the 1980s most of the occupants were males who were born on January 1st, 1950. Almost all of the vehicles involved in crashes were the 1960 model year.' Except they weren't. These were just the default settings. [@Criado_Perez_2019, p.190]

* Contradiction (for example, two databases with the same person but the date of birth differs, perhaps due to non-ISO8601 entry: 07-08-79 and 08-07-79 both have the same digits but one could be mm-dd-yy and the other dd-mm-yy...we just don't know which is the correct one. Or is one a typo?)


See [@Zumel_Mount_2019, chapters 3 and 4]

### Data quality

There is quite a lot of literature on how to define data quality; the concepts in the hierarchical dimensions described by Wang, Reddy and Kon ^[Richard Y. Wang, M.P. Reddy and Henry B. Kon, "Toward quality data: An attribute-based approach", _Decision Support Systems_, 13 (1995) 349-372] is as follows:

* Accessible

  - Available
  
* Interpretable

  - Syntax
  
  - Semantics
  
* Useful

  - Relevant
  
  - Timely
  
    - Current
    
    - Non-volatile
    
* Believable

  - Complete
  
  - Consistent
  
  - From a credible source
  
  - Accurate
  

"Dirty data" is data that falls short on the _believable_ dimension, in particular evaluating whether the data are complete, consistent, and accurate. 

**Complete**

For our purposes, "complete" means whether any values in each record are missing (internally complete).

Is it possible to determine whether the records are an accurate representation of the whole. With a well-designed sample, it is possible that a sub-set of the population can provide an accurate set of measures about the population. 

**Consistent**

We will consider a measure to be consistent if the same value is reported in the same way.

Some examples:

* units are consistent; temperature is consistently reporting in degrees Celsius, not mixing Farenheit and Kelvin, or income is in a single currency.

* spelling is consistent. "British Columbia", "B.C." and "BC". Or the 57 different ways to spell "Philadelphia". ^[In this tweet, you can see 57 different ways of spelling "Philadelphia" in the data collected in a US Government loan form (https://twitter.com/dataeditor/status/1280278987797942272?s=20)]

**Accurate**

(How to evaluate?)




If the data fails to meet our standards or quality, we need to _clean the data_. Which doesn't sound like a lot of fun. Didn't we want to be data scientists or business intelligence experts or academic scientists to uncover the insights hiding in the data? 

> The act of cleaning data imposes values/judgments/interpretations upon data intended to allow downstream analysis algorithms to function and give results. That’s exactly the same as doing data analysis. In fact, “cleaning” is just a spectrum of reusable data transformations on the path towards doing a full data analysis. —Randy Au, 2020-09-15 ^[["Data Cleaning IS Analysis, Not Grunt Work"](https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt)]


So what are we doing when we say we are "cleaning the data"? And how can we confirm that it is "cleaned" in the way that we have defined?


**The first challenge**: how do we find these things?

**The second challenge**: what can and should we do about them?


## Identifying dirty data

There are some approaches to finding problems in the data that can be automated (done programmatically). These are sometimes called "data validation" methods; they can be applied when the data is collected (for example, in a web-based survey tool) or after the data has been collected.

Validation while the data is being collected might be through the use of drop-down boxes that eliminates spelling mistakes, or that forces entries to match a particular pattern. For example, a Canadian postal code is always in the pattern of capital letter (with some exceptions), digit from 0 to 9, capital letter, space, digit, capital letter, digit. And all B.C. postal codes start with the letter "V", adding another dimension for possible validation. Electronic data collection can also enforce that an element is completed before submission. 

* All of these have risks, though. Forcing someone to complete a field might lead to made-up values, which might be worse than a missing field. Imagine forcing someone to report their income—they might enter a zero or a preposterously very large number, neither of them accurate.

For our work here, we will focus on validating data after is has been collected. The analyst can specify parameters prior to it being made available for the next steps. These can be expressed as rules or assumptions, and are based on knowledge of the subject. ^[Mark van der Loo and Edwin de Jonge, [_Statistical Data Cleaning with Applications in R_, Chapter 6: Data Validation](https://learning-oreilly-com.ezproxy.library.uvic.ca/library/view/statistical-data-cleaning/9781118897157/?ar?orpq&email=oJbr4Ho8VLXz0zbFzjLK2g%3d%3d&tstamp=1600297427&id=39EE56C0062775092344911C1657C64468AFDE9B)]

* A field such as postal code has to be structured in a certain way (this is where your regex skills come in)

* A value might have a certain range 

* Some values, such as age, have to be positive

* There might be a known relationship between two or more variables


### The {validate} package

As with other data analysis topics, people in the R user community have confronted the problem of dirty data, and have written and shared functions to help the rest of us. The {validate} package was written and is supported by two of the methodologists at Statistics Netherlands.

The first function we will investigate is `check_that`, using the data in {palmerpenguins}. We know that these data are nice and clean—they have already gone through a validation process.

```{r setup}
library(tidyverse)

library(palmerpenguins)
library(validate)

penguins

```


**Range of values**: we know, being experts on all things penguin, that gentoo penguins are the third largest of the penguin species, and can weigh up to 8.5 kg. So we might want to see if we have any records with penguins with a body mass of more than 8500 grams. The `check_that` function inverts this question; we are ensuring that all of the records have penguin body mass values that are less than or equal to 8500 grams. If the value is below that value, it returns a "pass". 


```{r}

check_penguins <- check_that(penguins, body_mass_g <= 8500)

summary(check_penguins)

```

What we see is that all of the penguins have a body mass of less than or equal to 8500 grams, and there are 2 "NA" values in the data.


**Relationship**: 

While the heaviest gentoo penguins can be up to 8.5 kg, it is very unusual to find an Adélie or Chinstrap penguin that weighs more than 5 kg. What happens if we change the body mass threshold to 5000 grams?

```{r}

check_penguins <- check_that(penguins, body_mass_g <= 5000)

summary(check_penguins)

```

In addition to telling us that there are 61 records where the value in the `body_mass_g` variable is greater than 5000 grams, it is also letting us know that there are 2 `NA` values.

We can add an `if()` statement that filters out the gentoo penguins, and then check the body mass.


```{r}

check_penguins <- check_that(penguins, 
                             if (species != "Gentoo") body_mass_g <= 5000)

summary(check_penguins)

```

Now all the records (except the single "NA") pass. What has happened? The first thing is that we have excluded all of the gentoo penguins—if it's gentoo, it gets marked "pass". All the remaining penguins (that is, the chinstrap and Adélie birds) are evaluated against the body mass value of 5000 grams. And they all pass.

Let's explore the data a bit more...if we filter our penguins by those that are over 5000 grams, what do we find?

```{r}
penguins %>%
  filter(body_mass_g > 5000) %>% 
  distinct(species)

```

So this is confirms that all of the heavier penguins are gentoo, meeting our expectations.


**Range:** Year

We know that the palmerpenguin data is from three seasons of measurement, 2007–2009. We can write a validation test to ensure that our `year` variable falls witin that range.

```{r}

check_penguins <- check_that(penguins, year >= 2007 & year <= 2009)
summary(check_penguins)

```

What if we check for a single year?

```{r}
check_penguins <- check_that(penguins, year == 2009)
summary(check_penguins)

```

The output gives us the number of cases in that year in the `passes` the test, and the number of cases that are not in that year in the `fails`.




**Relationship**: we know that some species have only been measured on one of the three islands. Gentoo penguins have only been sampled on Biscoe Island, and chinstrap penguins in our sample come only from Dream Island. Does the data reflect that?

In the first test (labelled `g`), if it's not a gentoo it gets included in the `passes`--and if it is a gentoo, it has to be on Biscoe Island to get a pass. In the `c` test, we will do the same for chinstrap penguins, which should all be on Dream Island.


```{r}

check_penguins <- check_that(penguins, 
                             g = if (species == "Gentoo") island == "Biscoe",
                             c = if  (species == "Chinstrap") island == "Dream")

summary(check_penguins)

```

Both tests have all of the cases pass, and none fail. 

We could have written code to generate a summary table to check this. In many datasets, however, there will be too many combinations to reliably investigate in this manual manner.

```{r}
penguins %>% 
  group_by(species, island) %>% 
  tally
```

A single test could be written using the "AND" operator `&`.

```{r}

check_penguins <- check_that(penguins, 
                             if (species == "Gentoo") island == "Biscoe" &
                             if  (species == "Chinstrap") island == "Dream")

summary(check_penguins)



```


We can also test against a list:

```{r}
island_list <- c("Biscoe", "Dream", "Torgersen")

check_penguins <- check_that(penguins, if (species == "Adelie") island %in% island_list)

summary(check_penguins)


```

_What happens if we fail to include Dream Island in our list of islands where Adélie penguins are found?_


{validator} also allows us to check variable types. Because R stores a dataframe column as a single type, this returns only one pass/fail evaluation.

```{r}

penguins

is.integer(penguins$year)

summary(check_that(penguins, is.integer(year)))

```

### badpenguins

I changed the penguins data by adding five fictional penguins, and added some measurement data for them. Let's load the data file "badpenguins.rds" (an R data file) and run a couple of the tests we used earlier:



```{r}
badpenguins <- read_rds(here::here("data", "badpenguins.rds"))

check_penguins <- check_that(badpenguins, year >= 2007 & year <= 2009)
summary(check_penguins)

```

```{r}
check_penguins <- check_that(badpenguins, island %in% island_list)
summary(check_penguins)
```

In both cases, we get 5 "fails". How can we check which records have failed? {validate} provides two other functions, `validator()` and `confront()`, which is a way to generatea detailed record-by-record account of which records have failed our test.

First, we assign our tests using `validator()`


```{r}

v <- validator(
  island %in% island_list,
  year >= 2007 & year <= 2009,
  body_mass_g <= 8500
)

```


```{r}

cf <- confront(badpenguins, v)

cf
```

Let's look at the last ten rows in our `badpenguins` data. Rows 340 through 344 are original to the clean `penguins` dataset; 345 through 349 are the ones I added. The first one fails on all 3 tests, while the others fail on the first two but have "NA" values for the remainder.

```{r}

tail(values(cf), 10)

```

We can identify each of the rows where we have a failed test using a filter:



## REFERENCE MATERIAL

You can find more details in _R for Data Science_:
https://r4ds.had.co.nz/data-import.html


{readr} 
https://readr.tidyverse.org/

{readxl}
https://readxl.tidyverse.org/index.html


