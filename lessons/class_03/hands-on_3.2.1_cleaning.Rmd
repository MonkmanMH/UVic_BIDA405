---
title: "Validation and cleaning"
subtitle: "3: cleaning"
output:
  html_document:
    df_print: paged
---

<!-- 
This file by Martin Monkman is licensed under a Creative Commons Attribution 4.0 International License. 
-->

```{r setup}
library(tidyverse)

```



## Cleaning dirty data

> Cleaning the data is a prerequisite to getting a rock solid understanding of the underlying data.
— Randy Au

The two things I took from Randy Au's recent article, ["Data Cleaning IS Analysis, Not Grunt Work"](https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt) (2020-09-15):

1. Data cleaning is not separate from analysis

2. Data cleaning requires judgment, which in turn requires subject matter knowledge and an understanding of the research question you are trying to answer. As the questions changes, so might the cleaning you need to undertake.

This leads to his conclusion that the "data cleaning" needs to be redefined as "building reusable transformations".


***

## Next up:

We have explored the data and found anomalies and missing values, run the validation tests to find cases that are outside the bounds of expected values, and now we have to make some decisions as to how we want to _clean_ the data. Being good data analysts, we know things like extreme cases and missing values can have an impact on our results.

The challenge with cleaning data is that it requires judgment. This judgment in turn requires someone with knowledge of the subject area and the research question to decide what needs to be cleaned, and how.

In his recent paper on data cleaning, Randy Au provides the example of a US Government form that generated multiple different spellings for the name of the city Philadelphia. But as he explains, and as is further developed in the paper by Rawson and Muñoz, in some research contexts there might be value in _not_ correcting the spelling.

This then leads to the conclusion that for one data set, what constitutes clean data might differ from one analytic question to another.

**Rule #1: keep the raw data!**


> The act of “cleaning data” is actually building a library of transformations that are largely reusable across multiple analyses on the same data set.
— Randy Au

**Rule #2: keep a record (documentation and code) of the transformations that were used to transform the data**


### Goals of data cleaning

(from Randy Au, "Data Cleaning IS Analysis, Not Grunt Work")

1. Fix things that will make your analysis algorithm choke

2. Reduce unwanted variation

3. Eliminate bias (where you can)

> We want to allow our analysis code to run, control useless variance, eliminate bias, and document for others to use, all in service to the array of potential analyses we want to run in the future.
— Randy Au

***

## Some cleaning methods

### Dealing with NA values

If you are using R, you'll find that some functions treat NA values as contagious: one NA in your calculation, and the whole thing won't run. Here's two short examples:

```{r}
# find mean of x
x <- c(1, 2, 3, 4, 5, NA)

mean(x)

# find the sum of x
sum(x)


```

We can solve this by adding the argument `na.rm = TRUE` to the function. This is a strategy that doesn't require explicitly cleaning the data, but it's worth noting that it reduces the number of valid cases that the calculation uses.

```{r}
# find mean of x
x <- c(1, 2, 3, 4, 5, NA)

mean(x, na.rm = TRUE)

# find the sum of x
sum(x, na.rm = TRUE)

# count the number of valid (not NA) cases in x
sum(!is.na(x))

```


**NOTE:** not all R functions treat NAs in this way! Some functions will quietly drop the NAs for you, which can sometimes be exactly what you want. But sometimes you want to know if there are NAs--make sure you check the documentation, since the creator of the package may have set `na.rm = TRUE` as the default. 



### Dealing with unusual cases

(adapted from _R for Data Science_, [Chapter 7: Exploratory Data Analysis](https://r4ds.had.co.nz/exploratory-data-analysis.html))


In the `diamonds` data set, there are some unusual values: some have a y axis dimension (the variable "y" in the data) of 0 mm, and some have values in the range of 30 and 60 mm. Zero is impossible, and a 30 mm diamond is BIG.

One strategy might be to filter these records out entirely:


```{r}
diamonds %>% 
  filter(y < 3 | y > 20)

diamonds2 <- diamonds %>% 
  filter(between(y, 3, 20))

# how many rows got filtered out?
nrow(diamonds) - nrow(diamonds2)
```

There are potential problems with the approach of removing records that have a missing value:

* it might remove many valid and useful values;

* it might leave too little data to be analyzed, particularly if there are many variables with missing values;

* it might introduce selection bias in the resulting data.

^[van der Loo & de Jonge, chapter 7]


An other option—which retains the all records, but drops those records from future calculations—is to replace the unusual values with NA.

Important note: this code creates a new dataframe, `diamonds2`

```{r}
diamonds2 <- diamonds %>% 
  mutate(y = ifelse(y < 3 | y > 20, NA, y))

```

An alternative syntax is to use the `case_when()` function (from the {dplyr} package; see https://dplyr.tidyverse.org/reference/case_when.html):

```{r}
diamonds2 <- diamonds %>% 
  mutate(y2 = case_when(
    y < 3 ~ NA_real_,
    y > 20 ~ NA_real_,
    TRUE ~ y
  ))

diamonds

diamonds2 %>% 
  filter(y < 3 | y > 20)
```

### Cleaning special codes

Often, a variable with have "unusual values" that are used intentionally. For example, in many surveys that use a Likert scale (the familiar "on a scale of 1 to 5, where 1 is Very Dissatisfied and 5 is Very Satisfied"), values like 98 and 99 are used to record "Don't Know" or "Not Applicable" responses.

If these are converted to NA values, the information in the value is lost. An alternate strategy is to create new variables that record this information. This type of variable is sometimes known as an _indicator variable_.

An example from _Practical Data Science with R_ shows us how this is done. The variable `gas_usage` is, for most records, a numeric value of gas usage. But there are also three special codes, the values 1, 2, and 3.

* 1 = "Gas bill included in rent or condo fee"

* 2 = "Gas bill included in electricity payment"

* 3 = "No charge or gas not used"

```{r}
customer_data <- readRDS("custdata.RDS")

customer_data %>% 
  group_by(gas_usage) %>% 
  tally()

```

```{r}

customer_data2 <- customer_data %>% 
  # create indicator variables
  mutate(gas_with_rent = (gas_usage == 1),
         gas_with_electricity = (gas_usage == 2),
         no_gas_bill = (gas_usage == 3)
         ) %>% 
  # convert the 1-3 values to NA
  mutate(gas_usage = ifelse(gas_usage < 4, NA, gas_usage))

customer_data2 %>% 
  group_by(gas_usage, gas_with_rent) %>% 
  tally()

```

These new indicator variables can be used in the analysis without compromising the information in the "gas_usage" variable..."gas_usage" now means gas usage.


### Replacing missing values

If you suspect that missing values are "missing at random"—that is, the reason they are missing has nothing to do with any of the other variables—you can replace the missing values with a reasonable estimate. Often, this is the mean of the value in that column. This doesn't change the average of the variable.



If you know that the missing variable has a relationship with other variables in your data, you can _impute_ an estimate of the missing value that is more accurate than the mean.

For example, if one of your variables is income, and you have a few missing cases, you could replace those missing values with the mean of income from the non-missing records. But income is often correlated with education and age, so you can run a regression model to estimate the impact of those two variables on income, and replace the missing income values with the estimated value from your regression model.

**Important note:** Use imputation with caution. It may sound like a great way to replace missing values, but if a variable's missingness is _not_ random (for example, older people may be less likely to report their income—retired people many not interpret their pension as "income", so resond with a "not applicable" answer), then imputation will potentially introduce bias into the data.

The methodologies behind different imputation methods are supported by a rich literature; I don't want to discourage you, but only to raise this as a flag. (For more on imputation, see van der Loo & de Jonge, chapter 10)
)

In this example, we will create two new variables, based on y, in the diamonds dataframe

```{r}
# y2: replace unusual values with NA
diamonds2 <- diamonds %>% 
  mutate(y2 = case_when(
    y < 3 ~ NA_real_,
    y > 20 ~ NA_real_,
    TRUE ~ y
  ))

# calculate the mean of y2 (remember to na.rm!)
y_mean <- mean(diamonds2$y2, na.rm = TRUE)

# y3: replace unusual values with mean of y2 (with dropped unusual values)
diamonds2 <- diamonds2 %>% 
  mutate(y3 = case_when(
    y < 3 ~ y_mean,
    y > 20 ~ y_mean,
    TRUE ~ y
  ))


diamonds2

diamonds2 %>% 
  filter(y < 3 | y > 20) %>% 
  select(carat, y, y2, y3)

```


### Removing duplicate records

One "dirty data" problem that sometimes occurs, particularly if data tables are combined, is that records end up duplicated. R has some functions to speed this cleaning.

```{r}
# Generate a vector 
set.seed(158)
x <- round(rnorm(20, 10, 5)) #random, normal distribution, 20 cases, mean of 10, s.d. of 5

x1 <- sort(x)

x1
duplicated(x1)

```

Note that the first instance of a number (e.g. the first 3) returns a FALSE, and the 2nd and subsequent instances return TRUE

Get the values of the duplicated entries

```{r}

# The values of the duplicated entries
# Note that '6' appears in the original vector three times, and so it has two
# entries here.
x[duplicated(x)]
#> [1] 10  3 11  8 10  8  5  6  6

# Duplicated entries, without repeats
unique(x[duplicated(x)])
#> [1] 10  3 11  8  5  6

# The original vector with all duplicates removed. These do the same:
unique(x)
#>  [1] 14 11  8  4 12  5 10  3  6  0 16
x[!duplicated(x)]
#>  [1] 14 11  8  4 12  5 10  3  6  0 16

```

(from http://www.cookbook-r.com/Manipulating_data/Finding_and_removing_duplicate_records/)


To remove duplicate rows in a data frame, we can use the `distinct` function in {dplyr}.

Here's a data table with the capital cities of the Canadian (Census Metropolitan Areas and Census Agglomerations) and their populations in 2019.

Our data table contains 2 duplicated records: the entire Edmonton row is repeated, and the Victoria row is repeated but with different population values (the smaller value is the population in 2018).

data source: https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1710013501

```{r}

can_city <- tribble(
  ~city, ~province, ~year, ~population,
  "Victoria", "BC", 2018, 396110,
  "Victoria", "BC", 2019, 402271,
  "Edmonton", "AB", 2019, 1447143,
  "Edmonton", "AB", 2019, 1447143,
  "Regina", "SK", 2019, 261684,
  "Winnipeg", "MB", 2019, 844566,
  "Toronto", "ON", 2019, 6471850,
  "Québec", "PQ", 2019, 824411,
  "Fredricton", "NB", 2019, 109883,
  "Halifax", "NS", 2019, 440348,
  "Charlottetown", "PE", 2019, 78568,
  "St. John's", "NL", 2019, 212433
)

head(can_city, 12)
```


We can use the `distinct()` function in {dplyr} to clean this.

To remove the entire duplicated rows (i.e. all columns), we can do the following:

```{r}
can_city %>% 
  distinct()
```

This removes the duplicated Edmonton row, but leaves both of the Victoria rows. 

We can specify the column we want to retain in our `distinct()` function:

```{r}

can_city %>% 
  distinct(city, .keep_all = TRUE)

```

To resolve the "first found, first kept" challenge, we might first filter by year, then apply the `distinct()` function.

```{r}

can_city %>%
  filter(year == 2019) %>% 
  distinct(city, .keep_all = TRUE)

```


## REFERENCE MATERIAL

Mark van der Loo and Edwin de Jonge, _Statistical Data Cleaning with Applications in R_ (2018):

* Chapter 6, "Data Validation" (sections 6.1-6.2)

* [UVic library link](https://learning-oreilly-com.ezproxy.library.uvic.ca/library/view/statistical-data-cleaning/9781118897157/c11.xhtml#c11)



Nina Zumel and John Mount _Practical Data Science with R (2nd edition)_:

* Chapter 3 "Exploring Data" and Chapter 4 "Managing Data"

* [UVic library link](https://learning-oreilly-com.ezproxy.library.uvic.ca/library/view/practical-data-science/9781617295874/?ar?orpq&email=oJbr4Ho8VLXz0zbFzjLK2g%3d%3d&tstamp=1600527352&id=DC7689B4AD2FA446468DD1CE0CCCD91B187CD9C4)


Hadley Wickham and Garrett Grolemund, _R for Data Science_, [Chapter 7: Exploratory Data Analysis](https://r4ds.had.co.nz/exploratory-data-analysis.html)

Randy Au, ["Data Cleaning IS Analysis, Not Grunt Work"](https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt), 2020-09-15


Katie Rawson and Trevor Muñoz, ["Against Cleaning"](https://dhdebates.gc.cuny.edu/read/untitled-f2acf72c-a469-49d8-be35-67f9ac1e3a60/section/07154de9-4903-428e-9c61-7a92a6f22e51)



{readr} 
https://readr.tidyverse.org/

{readxl}
https://readxl.tidyverse.org/index.html


